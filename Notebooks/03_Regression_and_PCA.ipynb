{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Can't find good material for this..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Can't find good material for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## PCA Algorithm Basics\n",
    "\n",
    "The PCA Algorithm relies heavily on the Spectral (eigenvalue) related properties of a matrix.\n",
    "\n",
    "Dumb question $ -$ what are the eigenvalues of a non-square matrix?\n",
    "\n",
    "Recall that we can think of $m$ features of $n$ samples as being represented by a $(m, n)$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.45694848  0.68215992  0.96632438  0.26512448  0.00704128  0.90255228\n",
      "   0.34997457  0.50455554  0.9734311   0.66443378]\n",
      " [ 0.04349394  0.26550274  0.61090397  0.96447047  0.70466755  0.23507757\n",
      "   0.0648513   0.4159312   0.43647392  0.59190231]\n",
      " [ 0.20310541  0.46173624  0.70207863  0.54076666  0.44357899  0.32408508\n",
      "   0.98588592  0.90023997  0.51181221  0.34424193]\n",
      " [ 0.65947621  0.5708275   0.45346432  0.03693914  0.40652308  0.92640225\n",
      "   0.18346825  0.97416679  0.40793972  0.39477877]\n",
      " [ 0.96187904  0.57048095  0.42738345  0.94862032  0.96681682  0.10518407\n",
      "   0.9696336   0.45898636  0.67795081  0.56169102]]\n"
     ]
    }
   ],
   "source": [
    "# Let us see what this would look like in numpy.\n",
    "\n",
    "# First make choose m and n such that m != n\n",
    "m = 5\n",
    "n = 10\n",
    "\n",
    "# Make the matrix A\n",
    "A = np.random.rand(m, n)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uh Oh we caused a linear algebra error!\n",
      "The last two dimensions must be square!\n",
      "This means we can't compute the eigenvalues of the matrix.\n"
     ]
    }
   ],
   "source": [
    "# Now compute its eigenvalues.\n",
    "try:\n",
    "    vals, vecs = np.linalg.eig(A)\n",
    "    print(vals)\n",
    "except:\n",
    "    print(\"Uh Oh we caused a linear algebra error!\")\n",
    "    print(\"The last two dimensions must be square!\")\n",
    "    print(\"This means we can't compute the eigenvalues of the matrix.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looks like we'll have to cheat a bit.\n",
    "\n",
    "I mean -- do something smart!\n",
    "\n",
    "Consider two matrices, A of shape $(m, n)$ and B of shape $(n, m)$.\n",
    "\n",
    "We can compute their product and wind up with a matrix of shape $(m, m)$ or $(n, n)$ -- depending on whether A or B comes first.\n",
    "\n",
    "Pop quiz!\n",
    "\n",
    "What is the first Matrix that comes to mind for an $(n, m)$ matrix that has shape $(m, n)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The TRANSPOSE OF IT!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of A is: (5, 10)\n",
      "A^T has shape: (10, 5)\n"
     ]
    }
   ],
   "source": [
    "# Let's double check that real fast.\n",
    "print(\"The shape of A is: {}\".format(A.shape))\n",
    "print(\"A^T has shape: {}\".format(A.transpose().shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "Last 2 dimensions of the array must be square",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-7fe8b4dfbc45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Let's see what the spectrum looks like.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mA_T\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_T\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ianandrich/miniconda3/lib/python3.5/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36meig\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m   1140\u001b[0m     \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_makearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m     \u001b[0m_assertRankAtLeast2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m     \u001b[0m_assertNdSquareness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1143\u001b[0m     \u001b[0m_assertFinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_commonType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ianandrich/miniconda3/lib/python3.5/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36m_assertNdSquareness\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Last 2 dimensions of the array must be square'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_assertFinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLinAlgError\u001b[0m: Last 2 dimensions of the array must be square"
     ]
    }
   ],
   "source": [
    "# Let's see what the spectrum looks like.\n",
    "A_T = A.transpose()\n",
    "vals, vecs = np.linalg.eig(A_T)\n",
    "print(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 15.22571032   1.93054281   0.78527359   0.45608969   0.51259026]\n"
     ]
    }
   ],
   "source": [
    "# Darn it it still isn't square!\n",
    "# What about.... A * A^T\n",
    "\n",
    "A_AT = np.matmul(A, A_T)\n",
    "vals, vecs = np.linalg.eig(A_AT)\n",
    "print(vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This.... actually makes sense.  Lets compare the other multiplication just to see whats going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.52257103e+01 +0.00000000e+00j   1.93054281e+00 +0.00000000e+00j\n",
      "   7.85273591e-01 +0.00000000e+00j   5.12590257e-01 +0.00000000e+00j\n",
      "   4.56089693e-01 +0.00000000e+00j   4.10956892e-16 +1.68308728e-16j\n",
      "   4.10956892e-16 -1.68308728e-16j   9.90483307e-17 +0.00000000e+00j\n",
      "  -4.46644672e-16 +0.00000000e+00j  -2.44978315e-16 +0.00000000e+00j]\n"
     ]
    }
   ],
   "source": [
    "AT_A = np.matmul(A_T, A)\n",
    "vals, vecs = np.linalg.eig(AT_A)\n",
    "print(vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Huh.... looks like they share some eigenvalues \n",
    "Look at the leading 5 numbers.\n",
    "\n",
    "#### And it seems like the rest are effectively 0 -- this is just a rounding error.  They are 0.\n",
    "#### And are they in descending order or is it just my imigination?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theorem 1:\n",
    "The matrices $A \\times A^T$ and $A^T \\times A$ share the same nonzero eigenvalues. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theorem 2:\n",
    "The matrices $A \\times A^T$ and $A^T \\times A$ have non-negative eigenvalues.\n",
    "\n",
    "\n",
    "#### Note:\n",
    "This actually follows from the fact that they are symmetric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemma 1:  (A Helper Theorem or important observation.)\n",
    "To translate an eigenvector of $A \\times A^T$ to an eigenvector of $A^T \\times A$ we simply left multiply the eigenvector $A \\times \\vec{v}$.  This holds the other way as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise try it!  Extract an eigenvector from A x A^T and left multiply it by A.\n",
    "# Check the resulting eigenvector is in A^T x A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "It seems like we can translate them between the two different representations without a problem.\n",
    "\n",
    "Thats good enough for me! \n",
    "\n",
    "We can consider the non-square matrix $A$ of shape $(5, 10)$ to have 5 eigenvalues.\n",
    "\n",
    "### WE ONLY NEED A NUMBER OF EIGENVALUES EQUAL TO THE NUMBER OF FEATURES!!!!\n",
    "We actually do not have to summarize our data as much as it seems like we might."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From here on out we need to define a few statistical Matrices.  These will be left as challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a mean vector $\\vec \\mu$ derived from the original A, which is a vector of the mean of each of the original features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a matrix B (m, n), such that each element of B is A - $\\vec \\mu$ applied to each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the covariance matrix $S \\colon = \\frac{1}{n-1} B \\times B^T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Why should the covariance matrix be a square matrix in the number of features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recall the Covariance Formular For Two Variables\n",
    "\n",
    "$Cov(A,B) = \\frac{1}{n-1}((a_1 - \\mu_A)(b_1-\\mu_B)+ ... + (a_n - \\mu_a)(b_n - \\mu_b))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Is the name Covariance Matrix justified?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# What are the values on the Diagonal of the Covariance Matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Trace of a Matrix\n",
    "\n",
    "$$Tr(A) = \\Sigma \\lambda_i$$\n",
    "The trace is the sum of the eigenvalues of a matrix.\n",
    "\n",
    "It can be alternatively stated as the sum of the values on the diagonal, but this is not obvious!!\n",
    "\n",
    "In our case, we know the values on the Diagonal are the Variance for the feature in that column/row.\n",
    "\n",
    "Therefore the $Tr(S)$ is just the total variance in the data set!\n",
    "\n",
    "The eigenvectors $\\vec v_i $ are the directions of maximum variance.\n",
    "The eigenvalues are the amount of variance in that direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_noisy_line(n_samples=50):\n",
    "    '''\n",
    "    This function generates a noisy line of slope 1 and returns the \n",
    "    matrix associated with these n_samples, with noise +- 1 from a \n",
    "    straight line.\n",
    "    \n",
    "    This matrix follows the convention that \n",
    "    rows are features, and columns are samples.\n",
    "    '''\n",
    "    \n",
    "    return matrix_A\n",
    "    \n",
    "def make_B_from_A(matrix_A):\n",
    "    '''\n",
    "    This function generates the B matrix from the sample matrix A.\n",
    "    '''\n",
    "    \n",
    "    return matrix_B\n",
    "    \n",
    "def make_S_from_B(matrix_B):\n",
    "    '''\n",
    "    This function generates the matrix S from B.\n",
    "    '''\n",
    "    \n",
    "    return matrix_S\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
